{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 (Data Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data : 150000\n",
      "test data : 50000\n"
     ]
    }
   ],
   "source": [
    "# data 로드하기(train, test), length 출력\n",
    "# 'ratings_train.txt', ratings_test.txt'\n",
    "train_data = pd.read_table('./data/ratings_train.txt')\n",
    "test_data = pd.read_table('./data/ratings_test.txt')\n",
    "\n",
    "print('train data :',len(train_data)) # 훈련용 리뷰 개수 출력\n",
    "print('test data :',len(test_data)) # 훈련용 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 살펴보기 (컬럼명, 중복항, 라벨 별 count, Null 값 유무)\n",
    "train_data.head()\n",
    "# document 중복항 살펴보기: 4000개\n",
    "train_data['document'].nunique()\n",
    "# 라벨 별 count 균형 1: 긍 / 0: 부\n",
    "train_data.groupby('label').size().reset_index(name = 'count')\n",
    "# Null 값 유무 확인\n",
    "train_data.isnull().values.any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146183\n",
      "False\n",
      "146182\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "# Document 컬럼의 중복 제거\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "print(len(train_data))\n",
    "\n",
    "# Document 컬럼의 Null 값 제거\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
       "1   3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document 컬럼의 한글과 공백 텍스트를 제외하고 모두 제거\n",
    "# 정규식: \"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\"\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 한글과 공백을 제외하고 모두 제거\n",
    "# import re\n",
    "# def remove_special_characters(text):\n",
    "#     # 한글과 공백을 제외한 모든 문자를 제거\n",
    "#     return re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", text)\n",
    "# train_data['document'] = train_data['document'].apply(remove_special_chracters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0\n",
      "document    0\n",
      "label       0\n",
      "dtype: int64\n",
      "최종 train 데이터 샘플 수 145393\n"
     ]
    }
   ],
   "source": [
    "# 한글 외 내용을 제거한 뒤 빈문자열이 발생할 수 있음 (영어로만, 특수기호로만 이루어진 문장)\n",
    "# 공백 데이터 치환 정규식: '^ +'\n",
    "# 치환 후 Null 값으로 변경\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\", regex=True) # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# Null 값 제거\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "print(\"최종 train 데이터 샘플 수:\",len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 test 데이터 샘플 수: 48852\n"
     ]
    }
   ],
   "source": [
    "# Test 데이터 적용\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 정규 표현식 수행\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\", regex=True) # 공백은 empty 값으로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "print(\"최종 test 데이터 샘플 수:\",len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:17<00:00, 222.75it/s]\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['은', '는', '이', '가', '을', '를', '의', '한', '에', '하', '고', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '도', '는', '다']\n",
    "\n",
    "# train 데이터 형태소 분석 및 전처리: 4000개만\n",
    "X_train = []\n",
    "okt = Okt()\n",
    "for sentence in tqdm(train_data['document'][:4000]):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)\n",
    "# 전문 약 15분 소요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mecab 사용을 원할 시 colab 권장\n",
    "# !git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "# cd Mecab-ko-for-Google-Colab\n",
    "# !bash install_mecab-ko_on_colab_light_220429.sh\n",
    "# from konlpy.tag import Mecab\n",
    "# mecab = Mecab()\n",
    "# mecab.morphs(\"안녕하세요~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['아', '더빙', '진짜', '짜증나다', '목소리'], ['흠', '포스터', '보고', '초딩', '영화', '줄', '오버', '연기', '조차', '가볍다', '않다'], ['너', '무재', '밓었', '다그', '래서', '보다', '추천'], ['교도소', '이야기', '구먼', '솔직하다', '재미', '없다', '평점', '조정'], ['사이', '몬페', '그', '익살스럽다', '연기', '돋보이다', '영화', '스파이더맨', '에서', '늙다', '보이다', '하다', '커스틴', '던스트', '너무나도', '이쁘다', '보이다']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:08<00:00, 241.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# test 데이터 형태소 분석 및 전처리: 2000개만\n",
    "X_test = []\n",
    "for sentence in tqdm(test_data['document'][:2000]):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['굳다', 'ㅋ'], ['뭐', '야', '평점', '나쁘다', '않다', '점', '짜다', '리', '더', '더욱', '아니다'], ['지루하다', '않다', '완전', '막장', '돈', '주다', '보기', '에는'], ['만', '아니다', '별', '다섯', '개', '주다', '왜', '로', '나오다', '제', '심기', '불편하다', '하다'], ['음악', '주가', '되다', '최고', '음악', '영화']]\n"
     ]
    }
   ],
   "source": [
    "# 전처리 된 test 데이터 출력\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터를 train, valid 데이터로 나누기 (8:2)\n",
    "y_train = np.array(train_data['label'][:4000])\n",
    "y_test = np.array(test_data['label'][:2000])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0, stratify=y_train)\n",
    "# stratify: 클래스 비율 조절 (train/valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 데이터 클래스 비율\n",
      "부정 = 50.062%\n",
      "긍정 = 49.938%\n",
      "\n",
      "valid 데이터 클래스 비율\n",
      "부정 = 50.125%\n",
      "긍정 = 49.875%\n",
      "\n",
      "test 데이터의 클래스 비율\n",
      "부정 = 48.75%\n",
      "긍정 = 51.25%\n"
     ]
    }
   ],
   "source": [
    "# 데이터 별 클래스 비율 출력하기 (Train, valid, test)\n",
    "print('train 데이터 클래스 비율')\n",
    "print(f'부정 = {round(np.sum(y_train==0)/len(y_train) * 100,3)}%')\n",
    "print(f'긍정 = {round(np.sum(y_train==1)/len(y_train) * 100,3)}%')\n",
    "print()\n",
    "print('valid 데이터 클래스 비율')\n",
    "print(f'부정 = {round(np.sum(y_valid==0)/len(y_valid) * 100,3)}%')\n",
    "print(f'긍정 = {round(np.sum(y_valid==1)/len(y_valid) * 100,3)}%')\n",
    "print()\n",
    "print('test 데이터의 클래스 비율')\n",
    "print(f'부정 = {round(np.sum(y_test==0)/len(y_test) * 100,3)}%')\n",
    "print(f'긍정 = {round(np.count_nonzero(y_test)/len(y_test) * 100,3)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어수 : 6715\n"
     ]
    }
   ],
   "source": [
    "# 전체 단어수(Unique) 세기 : X_train 기준\n",
    "word_list = []\n",
    "for sent in X_train:\n",
    "    for word in sent:\n",
    "      word_list.append(word)\n",
    "\n",
    "word_counts = Counter(word_list) # list, set 해도 동일한 결과 나옴\n",
    "print('총 단어수 :', len(word_counts))\n",
    "# print(word_counts)\n",
    "# print(word_counts['영화'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['영화', '하다', '보다', '없다', '이다', '있다', '좋다', '정말', '재밌다', '너무']\n"
     ]
    }
   ],
   "source": [
    "# 빈도수 기준으로 상위 10개 출력\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 voca의 크기 : 6715\n",
      "등장 빈도가 N 이하인 단어의 수: 4799\n",
      "전체 단어 중 등장 빈도가 N 이하인 단어의 비율: 71.46686522710351\n",
      "전체 등장 빈도에서 N 이하인 단어 등장 빈도 비율: 16.45401307041616\n"
     ]
    }
   ],
   "source": [
    "# 단어 수, 등장 빈도 N번 미만 단어 수, 비율 등 계산 및 출력\n",
    "threshold = 3\n",
    "total_cnt = len(word_counts) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in word_counts.items():\n",
    "    total_freq += value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt += 1\n",
    "        rare_freq += value\n",
    "\n",
    "print(\"단어 voca의 크기 :\", total_cnt)\n",
    "print(\"등장 빈도가 N 이하인 단어의 수:\", rare_cnt)\n",
    "print(\"전체 단어 중 등장 빈도가 N 이하인 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 N 이하인 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어수 : 1916\n"
     ]
    }
   ],
   "source": [
    "# vocab에서 빈도수 2이하인 단어는 제거\n",
    "vocab_size = total_cnt - rare_cnt\n",
    "vocab = vocab[:vocab_size]\n",
    "print('총 단어수 :', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어수 : 1918\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "# 최종 Voca dictionary 구성하기\n",
    "# Speical token\n",
    "word_to_index = {}\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<UNK>'] = 1\n",
    "\n",
    "# 단어 dictionary 만들기 {pad:0, unk:1, word:2, word:3}\n",
    "for index, word in enumerate(vocab) :\n",
    "  word_to_index[word] = index + 2\n",
    "vocab_size = len(word_to_index)\n",
    "print('총 단어수 :', vocab_size)\n",
    "# print(word_to_index['마음'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩 method: 데이터를 dictionary를 통해 변환\n",
    "def texts_to_sequences(tokenized_X_data, word_to_index):\n",
    "  encoded_X_data = []\n",
    "  for sent in tokenized_X_data:\n",
    "    index_sequences = []\n",
    "    for word in sent:\n",
    "      try:\n",
    "          index_sequences.append(word_to_index[word])\n",
    "      except KeyError:\n",
    "          index_sequences.append(word_to_index['<UNK>'])\n",
    "    encoded_X_data.append(index_sequences)\n",
    "  return encoded_X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, valid, test 데이터 변환\n",
    "encoded_X_train = texts_to_sequences(X_train, word_to_index)\n",
    "encoded_X_valid = texts_to_sequences(X_valid, word_to_index)\n",
    "encoded_X_test = texts_to_sequences(X_test, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 12, 29, 72, 173, 611, 458, 1459, 58, 164, 3, 748, 1, 1460, 128]\n"
     ]
    }
   ],
   "source": [
    "# 인코딩 된 문장 출력해 보기\n",
    "print(encoded_X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코딩 Dictionary 만들기\n",
    "# word_to_index의 reverse된 dictionary 구성: {idx: words}\n",
    "index_to_word = {}\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 : ['주인공', '같다', '사람', '작품', '성', '뛰어나다', '영화인', '지는', '모르다', '이해', '하다', '어렵다', '고저', '한텐', '별로']\n",
      "디코딩 결과 : ['주인공', '같다', '사람', '작품', '성', '뛰어나다', '영화인', '지는', '모르다', '이해', '하다', '어렵다', '<UNK>', '한텐', '별로']\n"
     ]
    }
   ],
   "source": [
    "# 인코딩 된 문장 디코딩 하기, 출력\n",
    "decoded_sample = [index_to_word[word] for word in encoded_X_train[1]]\n",
    "print('샘플 :', X_train[1])\n",
    "print('디코딩 결과 :', decoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 최대 길이 : 54\n",
      "문서의 평균 길이 : 11.0459375\n"
     ]
    }
   ],
   "source": [
    "# padding : 정해진 길이(max_length)에 따라 문서의 길이를 맞춰주는(split or padding) 과정\n",
    "print('문서의 최대 길이 :', max(len(review) for review in encoded_X_train))\n",
    "print('문서의 평균 길이 :', sum(map(len, encoded_X_train))/len(encoded_X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 30 이하인 샘플의 비율: 93.875\n"
     ]
    }
   ],
   "source": [
    "# 전체 샘플 중 길이가 max_length 이하인 샘플의 비율(X_train 기준)\n",
    "max_len = 30 # 임의의 max_length 지정\n",
    "count = 0\n",
    "for sentence in X_train:\n",
    "    if(len(sentence) <= max_len):\n",
    "        count = count + 1\n",
    "print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(X_train))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : (3200, 30)\n",
      "검증 데이터의 크기 : (800, 30)\n",
      "테스트 데이터의 크기 : (2000, 30)\n"
     ]
    }
   ],
   "source": [
    "# train, valid, test에 대해 padding 진행: max_length에 맞춰 단어가 없는 구간에 0을 채워 넣는다\n",
    "def pad_sequences(sentences, max_len):\n",
    "  features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "  for index, sentence in enumerate(sentences):\n",
    "    if len(sentence) != 0:\n",
    "      features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "  return features\n",
    "\n",
    "padded_X_train = pad_sequences(encoded_X_train, max_len=max_len)\n",
    "padded_X_valid = pad_sequences(encoded_X_valid, max_len=max_len)\n",
    "padded_X_test = pad_sequences(encoded_X_test, max_len=max_len)\n",
    "\n",
    "print('train 데이터의 크기 :', padded_X_train.shape)\n",
    "print('valid 데이터의 크기 :', padded_X_valid.shape)\n",
    "print('test 데이터의 크기 :', padded_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "길이 : 30\n",
      "샘플 : [ 27  46 678 868 404 610   1   3 999   4   1 172   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# 패딩 처리 된 데이터 출력해 보기\n",
    "print('길이 :', len(padded_X_train[0]))\n",
    "print('샘플 :', padded_X_train[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "305494ec69d5ad97a583cc76e8fd52e450123bc765c435a27726a289dbe2d5e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
